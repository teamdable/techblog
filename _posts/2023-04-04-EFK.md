---
layout: post
title: 'EFK 구축 기초'
date: 2022-04-04 01:00:00 +0900
author: Kyungho Kang
tags: [ EFK, Fleuntbit, Fluentd ]
---

# EFK Guide

안녕하세요. 데이블 Infra 팀 강경호입니다.

데이블은 Kubernetes 컨테이너에서 생성하는 로그들을 활용하기 위해 EFK 스택을 사용하고 있습니다.

EFK는 ElasticSearch, Fluentd, Kibana의 약자입니다. 이 세 가지 오픈 소스 소프트웨어는 로그 수집, 저장 및 분석을 위한 강력한 도구들입니다.

ElasticSearch는 실시간으로 검색, 분석 및 데이터 저장이 가능한 분산 검색 엔진입니다.
Fluentd는 로그 데이터를 수집하고 다른 시스템으로 전송하는 데 사용되는 데이터 수집기입니다.
Kibana는 ElasticSearch에서 저장된 데이터를 시각화하고 검색하는 데 사용되는 데이터 분석 및 시각화 도구입니다.
EFK 스택은 이러한 각각의 도구들을 함께 사용하여, 분산 시스템에서 발생하는 로그 데이터를 실시간으로 수집, 저장, 분석 및 시각화할 수 있습니다. 이러한 기능은 로그 분석 및 디버깅 등의 목적으로 매우 유용합니다.

오늘은 데이블 인프라팀에서 EFK 구축 했던 내용을 공유드리려고 합니다.

# Why

---

Kubernetes 의 컨테이너 런타임은 생성된 모든 출력을 처리하고 컨테이너화된 애플리케이션 `stdout`및 `stderr` 스트림으로 리디렉션합니다. 
이렇게 생성된 로그는 `kubectl logs` 를 통해 확인할 수 있습니다.
하지만 POD수가 늘어날 수록 생성되는 모든 로그를 kubectl logs 를 이용해서 확인하기에는 한계가 있습니다.

또한 로그가 POD내에 저장되는 것에는 한계가 있습니다.
로그 파일의 최대 크기와 갯수는 kubelet 의 설정된 containerLogMax,SizecontainerLogMaxFiles 값에 의해 결정되며 이 설정값을 넘길 경우 오래된 로그들 부터 삭제 됩니다.
물론 POD가 제거될 경우 저장되어 있던 로그들도 함께 삭제되어 더 이상 확인할 수 없습니다.

이런 한계를 극복하고 로그의 접근성과 활용도를 높이기 위해 EFK 스택을 사용하여 로그를 수집, 저장, 제공하고 있습니다.

# How

---

Fluent Bit 에서 사용하는 모든 로그나 메트릭을 이벤트 혹은 레코드라고 합니다.

이번 글에서는 이벤트라는 용어를 사용하도록 하겠습니다.

Fluent Bit 의 Data Pipeline 에는 INPUT, PARSER, FILTER, OUTPUT 이 있습니다.
여기서 이야기하는 Data Pipeline 이란 로그 수집부터 전송까지의 모든 단계를 뜻합니다.

### INPUT

INPUT 은 어떤 이벤트를 어떻게 수집할지에 대한 내용입니다.

```bash
[INPUT]
    Name              tail
    Tag               kube.*
    Path              /var/log/containers/*.log
    multiline.parser  docker, cri
```

- `Name`
    - INPUT 플러그인을 지정하는 항목입니다.
    - 주로 사용하는 플러그인으로는 Tail 이 있습니다.
    Tail 은 하나 이상의 텍스트 파일을 모니터링하는 것으로 Shell 의 tail -f 명령과 유사합니다.
    - 더 많은 플러그인은 [여기](https://docs.fluentbit.io/manual/pipeline/inputs)에서 찾아볼 수 있습니다.
- `Tag`
    - Fluent Bit 에 들어오는 모든 이벤트에는 태그가 할당됩니다.
    Fluent Bit 은 이 태그값을 기준으로 FILTER 또는 OUTPUT 로 라우팅을 진행합니다.
    매칭 되는 태그가 없다면 해당 이벤트는 폐기됩니다.
    - 태그가 지정되지 않은 경우 해당 이벤트가 생성된 입력 플러그인 인스턴스의 이름을 할당합니다.
    - 
- `Path`
    - 특정 로그 파일 또는 여러 로그 파일을 지정하는 패턴입니다.
    - Kubenetes 에서 각 POD 는 Node 에 아래와 같이 저장됩니다.
        
        `/var/log/containers/*podName*_*namespace*_*containerName*-*containerID*`
        
    - Fluent Bit 은 각 노드에 [DaemonSet](https://kubernetes.io/ko/docs/concepts/workloads/controllers/daemonset/) 형태로 설치되며 노드의 `/var/log/` 경로에 대한 Volume 접근 권한을 가집니다.
    - 따라서 Fluent Bit 은 노드의 로그 경로와 동일한 경로로 로그를 수집할 수 있습니다.
    - 또한 아스트릭 기호를 지원하기 때문에 `/var/log/containers/*` 와 같이 설정할 수 있습니다.
    - 만약 아래와 같은 특정 컨테이너의 로그를 수집하고자 할때는 아래와 같이 설정할 수 있습니다.
        
        ```bash
        kind: Deployment
        metadata:
          name: apache-logs-annotated
          namespace: default
        spec:
          containers:
            name : apache
        ```
        
        `/var/log/container/apache-logs-annotated*_default_apache*`
        
        kube.var.log.container.apche-logs-annotated
        
- `multiline.parser`
    - 멀티라인 파서를 어떤 것으로 지정할 지 명시하는 항목입니다.
    - 보통 로그는 한 줄만 남는 경우도 있지만, 에러 로그 같은 경우 수십 줄의 로그가 남는 경우가 많습니다.
    이런 경우 멀티라인 파서를 적용하지 않게 되면 수십 줄의 로그가 각각 다른 이벤트로 저장됩니다.
    - Fluent Bit 은 기본적으로 docker, cri 등 의 파서를 제공합니다. 자세한 내용은 [여기](https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/multiline-parsing#built-in-multiline-parsers)를 참조바랍니다.

### FILTER

이벤트 콘텐츠를 수정, 변경  또는 삭제하는 단계입니다.

먼저 Kubernetes FILTER 에 대해 알아보겠습니다.

```bash
[FILTER]
        Name                kubernetes
        Match               kube.*
```

- `Name`
    - FILTER 플러그인 지정 항목입니다. ( [FILTER 플러그인 종류](https://docs.fluentbit.io/manual/pipeline/filters) )
    - 저희는 오늘 kubernetes 에서의 사용 방법을 진행하기 때문에 kubernetes 를 지정합니다.
    kubernetes 플러그인은 두가지 작업을 진행합니다.
        1. 이벤트의 태그에서 POD 의 이름, 네임스페이스, 컨테이너명, 컨테이너ID 를 추출합니다
        2. kubernetes apiserver 로 POD 의 ID, Label, Annotation 을 요청합니다.
            1. Fluent Bit 의 기본 kube-apiserver 주소는 [https://kubernetes.default.svc:443](https://kubernetes.default.svc:443) 로 만약 다른 주소를 사용하고 있다면 Kube_URL 값을 변경해야 합니다.
- `Match`
    - INPUT 단계에서 지정한 Tag 값과 비교하여 일치하는 이벤트만 처리합니다.
    - INPUT 에서 들어온 로그의 경로는 아래와 같은 Tag 변경됩니다.
        
        ```bash
        # 실제 로그 파일의 경로
        /var/log/container/apache-logs-annotated_default_apache-aeeccc7a9f00f6e4e066aeff0434cf80621215071f1b20a51e8340aa7c35eac6.log
        
        # 변환된 태그
        kube.var.log.containers.apache-logs-annotated_default_apache-aeeccc7a9f00f6e4e066aeff0434cf80621215071f1b20a51e8340aa7c35eac6.log
        ```
        
    - 따라서 해당 컨테이너의 로그만 이 FILTER 를 수행하고 싶다면 아래와 같이 설정할 수 있습니다.
    `kube.var.log.containers.apache-logs-annotated_default_apache-`
- `Kube_Tag_Prefix`
    - 태그가 너무 길다 생각이 되면 Kube_Tag_Prefix 를 설정할 수 있습니다.
    
    ```bash
    [FILTER]
            Kube_Tag_Prefix kube.var.log.containers.
    ```
    
    ```bash
    # 기존 태그
    kube.var.log.containers.apache-logs-annotated_default_apache-aeeccc7a9f00f6e4e066aeff0434cf80621215071f1b20a51e8340aa7c35eac6.log
    
    # 변환된 태그
    apache-logs-annotated_default_apache-aeeccc7a9f00f6e4e066aeff0434cf80621215071f1b20a51e8340aa7c35eac6.log
    ```
    

그 다음은 Multi Line Filter 입니다.
INPUT 단계에서 멀티라인을 적용하기 위해 Fluent bit 내장 파서를 사용했다면
여기서는 우리의 로그 타입에 맞게 멀티라인을 지정하는 것입니다.

```bash
[FILTER]
        name                  multiline
        match                 kube.*
        multiline.key_content log
        multiline.parser      multiline-regex
```

- `multiline.parser`
    - 멀티라인 파서의 이름을 지정하는 부분으로 parsers.conf 에 지정한 multiline-regex 를 사용합니다.
    
    ```bash
    [MULTILINE_PARSER]
        name          multiline-regex
        type          regex
        flush_timeout 1000
        key_content   log
        # ------|---------------|--------------------------------------------
        # rules |   state name  | regex pattern                  | next state
        # ------|---------------|--------------------------------------------
    		rule      "start_state"   "/\[\d+-\d+-\d+T\d+:\d+:\d+.\d+\](.*)/"   "cont"
        rule      "cont"          "/(^\s(.*)|}(.*))/"                       "cont"
    ```
    

### OUTPUT

```bash
[OUTPUT]
    Name                es
    Host                << ES Endpoint >>
    Port                443
    tls                 on
    Logstash_Format     on
    Logstash_DateFormat %Y-%m-%d
    Match               *
    Logstash_Prefix     << Index Name >>
```

최종적으로 정제한 로그를 전송하는 단계입니다.

동일하게 Name 으로 종류를 선택하고 Match 하는 로그를 전송하는 역할을 합니다

- `Logstash_Prefix`, `Logstash_Format`, `Logstash_DateFormat`
    - Log 전송 시, Index 뒤에 붙을 내용을 포맷팅하는 부분입니다
    - 예를 들어 아래와 같이 설정이 되어 있다면
    `Logstash_Prefix` : testIndex
    `Logstash_Format`: on
    `Logstash_DateFormat`: %Y-%m-%d
    실제 인덱스는 `testIndex-YYYY-MM-DD` 형식으로 저장되게 됩니다

+ 추가

Kibana 에서 로그를 보려면 Index 를 Stack 에 추가해야 합니다
추가하는 방법은 아래와 같습니다.

1. Kibana 페이지좌측 상단의 햄버거 메뉴
2. Stack Management
3. Index Patterns
4. Create index pattern
5. Index pattern name 에 추가할 인덱스 패턴을 입력 후 Next sterp 
(ex. testIndex-*)
6. Time field 에 time 을 선택합니다
(선택하지 않으면 로그가 시간순으로 보이지 않게 됩니다)
7. Create iundex pattern
![Kibana](/techblog/assets/images/Kibana/kibana.png)

# 참고

---

[https://kubernetes.io/docs/concepts/cluster-administration/logging/](https://kubernetes.io/docs/concepts/cluster-administration/logging/)

[https://docs.fluentbit.io/manual/about/what-is-fluent-bit](https://docs.fluentbit.io/manual/)

[https://aws.amazon.com/ko/blogs/containers/kubernetes-logging-powered-by-aws-for-fluent-bit/](https://aws.amazon.com/ko/blogs/containers/kubernetes-logging-powered-by-aws-for-fluent-bit/)


추가로 ElasticSearch 에서 Retention 을 지정할 수 있는 ILM (Index Lifecycle Management) 에 관한 내용과

현재 도입 중인 Datadog 과 EFK 의 비교글은 다음글에서 소개하도록 하겠습니다.

이상 데이블 강경호이었습니다.

감사합니다.