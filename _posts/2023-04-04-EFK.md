---
layout: post
title: 'EFK 구축 기초'
date: 2022-04-04 01:00:00 +0900
author: Kyungho Kang
tags: [ EFK, Fleuntbit, Fluentd ]
---

# EFK Guide

안녕하세요. 데이블 Infra 팀 강경호입니다.

오늘은 데이블에서 EFK 구축 했던 내용을 공유드리려고 합니다.

# Why

---

Kubernetes 의 컨테이너 런타임은 생성된 모든 출력을 처리하고 컨테이너화된 애플리케이션 `stdout`및 `stderr` 스트림으로 리디렉션합니다. 
이렇게 생성된 로그는 `kubectl logs` 를 통해 확인할 수 있습니다.
하지만 POD 의 수가 100개 이상이 된다면 `kubectl logs` 를 통해 볼 수 있는 로그는 한정적이게 됩니다.

또한 POD 의 로그는 kubelet 의 `containerLogMax`,`SizecontainerLogMaxFiles` 설정에 의해 각 로그 파일의 최대 크기와 허용되는 최대 파일 수 만큼 저장되게 됩니다. 하지만 설정값보다 더 많은 로그가 발생할 경우 이전에 생성된 로그가 제거되며, 만약 POD 가 노드에서 제거될 경우 해당하는 모든 컨테이너의 로그도 함께 제거됩니다.

따라서 이렇게 생성된 로그를 수집, 저장, 시각화를 위해 EFK Stack 을 도입하게 되었습니다.

# How

---

Fluent Bit 에서 사용하는 모든 로그나 메트릭을 이벤트 혹은 레코드라고 합니다.

이번 글에서는 이벤트라는 용어를 사용하도록 하겠습니다.

Fluent Bit 의 Pipeline 에는 INPUT, PARSER, FILTER, OUTPUT 이 있습니다.

### INPUT

INPUT 은 어떤 이벤트를 어떻게 수집할지에 대한 내용입니다.

```bash
[INPUT]
    Name              tail
    Tag               kube.*
    Path              /var/log/containers/*.log
    multiline.parser  docker, cri
```

- `Name`
    - INPUT 플러그인을 지정하는 항목입니다.
    - 주로 사용하는 플러그인으로는 Tail 이 있습니다.
    Tail 은 하나 이상의 텍스트 파일을 모니터링하는 것으로 Shell 의 tail -f 명령과 유사합니다.
    - 더 많은 플러그인은 [여기](https://docs.fluentbit.io/manual/pipeline/inputs)를 참조바랍니다.
- `Tag`
    - Fluent Bit 에 들어오는 모든 이벤트에는 태그가 할당됩니다.
    Fluent Bit 은 이 태그값을 기준으로 FILTER 또는 OUTPUT 로 라우팅을 진행합니다.
    매칭 되는 태그가 없다면 해당 이벤트는 폐기됩니다.
    - 태그가 지정되지 않은 경우 해당 이벤트가 생성된 입력 플러그인 인스턴스의 이름을 할당합니다.
    - 
- `Path`
    - 특정 로그 파일 또는 여러 로그 파일을 지정하는 패턴입니다.
    - Kubenetes 에서 각 POD 는 Node 에 아래와 같이 저장됩니다.
        
        `/var/log/containers/*podName*_*namespace*_*containerName*-*containerID*`
        
    - Fluent Bit 은 각 노드에 [DaemonSet](https://kubernetes.io/ko/docs/concepts/workloads/controllers/daemonset/) 형태로 설치되며 노드의 `/var/log/` 경로에 대한 Volume 접근 권한을 가집니다.
    - 따라서 Fluent Bit 은 노드의 로그 경로와 동일한 경로로 로그를 수집할 수 있습니다.
    - 또한 아스트릭 기호를 지원하기 때문에 `/var/log/containers/*` 와 같이 설정할 수 있습니다.
    - 만약 아래와 같은 특정 컨테이너의 로그를 수집하고자 할때는 아래와 같이 설정할 수 있습니다.
        
        ```bash
        kind: Deployment
        metadata:
          name: apache-logs-annotated
          namespace: default
        spec:
          containers:
            name : apache
        ```
        
        `/var/log/container/apache-logs-annotated*_default_apache*`
        
        kube.var.log.container.apche-logs-annotated
        
- `multiline.parser`
    - 멀티라인 파서를 어떤 것으로 지정할 지 명시하는 항목입니다.
    - 보통 로그는 한 줄만 남는 경우도 있지만, 에러 로그 같은 경우 수십 줄의 로그가 남는 경우가 많습니다.
    이런 경우 멀티라인 파서를 적용하지 않게 되면 수십 줄의 로그가 각각 다른 이벤트로 저장됩니다.
    - Fluent Bit 은 기본적으로 docker, cri 등 의 파서를 제공합니다. 자세한 내용은 [여기](https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/multiline-parsing#built-in-multiline-parsers)를 참조바랍니다.

### FILTER

이벤트 콘텐츠를 수정, 변경  또는 삭제하는 단계입니다.

먼저 Kubernetes FILTER 에 대해 알아보겠습니다.

```bash
[FILTER]
        Name                kubernetes
        Match               kube.*
```

- `Name`
    - FILTER 플러그인 지정 항목입니다. ( [FILTER 플러그인 종류](https://docs.fluentbit.io/manual/pipeline/filters) )
    - 저희는 오늘 kubernetes 에서의 사용 방법을 진행하기 때문에 kubernetes 를 지정합니다.
    kubernetes 플러그인은 두가지 작업을 진행합니다.
        1. 이벤트의 태그에서 POD 의 이름, 네임스페이스, 컨테이너명, 컨테이너ID 를 추출합니다
        2. kubernetes apiserver 로 POD 의 ID, Label, Annotation 을 요청합니다.
            1. Fluent Bit 의 기본 kube-apiserver 주소는 [https://kubernetes.default.svc:443](https://kubernetes.default.svc:443) 로 만약 다른 주소를 사용하고 있다면 Kube_URL 값을 변경해야 합니다.
- `Match`
    - INPUT 단계에서 지정한 Tag 값과 비교하여 일치하는 이벤트만 처리합니다.
    - INPUT 에서 들어온 로그의 경로는 아래와 같은 Tag 변경됩니다.
        
        ```bash
        # 실제 로그 파일의 경로
        /var/log/container/apache-logs-annotated_default_apache-aeeccc7a9f00f6e4e066aeff0434cf80621215071f1b20a51e8340aa7c35eac6.log
        
        # 변환된 태그
        kube.var.log.containers.apache-logs-annotated_default_apache-aeeccc7a9f00f6e4e066aeff0434cf80621215071f1b20a51e8340aa7c35eac6.log
        ```
        
    - 따라서 해당 컨테이너의 로그만 이 FILTER 를 수행하고 싶다면 아래와 같이 설정할 수 있습니다.
    `kube.var.log.containers.apache-logs-annotated_default_apache-`
- `Kube_Tag_Prefix`
    - 태그가 너무 길다 생각이 되면 Kube_Tag_Prefix 를 설정할 수 있습니다.
    
    ```bash
    [FILTER]
            Kube_Tag_Prefix kube.var.log.containers.
    ```
    
    ```bash
    # 기존 태그
    kube.var.log.containers.apache-logs-annotated_default_apache-aeeccc7a9f00f6e4e066aeff0434cf80621215071f1b20a51e8340aa7c35eac6.log
    
    # 변환된 태그
    apache-logs-annotated_default_apache-aeeccc7a9f00f6e4e066aeff0434cf80621215071f1b20a51e8340aa7c35eac6.log
    ```
    

그 다음은 Multi Line Filter 입니다.
INPUT 단계에서 멀티라인을 적용하기 위해 Fluent bit 내장 파서를 사용했다면
여기서는 우리의 로그 타입에 맞게 멀티라인을 지정하는 것입니다.

```bash
[FILTER]
        name                  multiline
        match                 kube.*
        multiline.key_content log
        multiline.parser      multiline-regex
```

- `multiline.parser`
    - 멀티라인 파서의 이름을 지정하는 부분으로 parsers.conf 에 지정한 multiline-regex 를 사용합니다.
    
    ```bash
    [MULTILINE_PARSER]
        name          multiline-regex
        type          regex
        flush_timeout 1000
        key_content   log
        # ------|---------------|--------------------------------------------
        # rules |   state name  | regex pattern                  | next state
        # ------|---------------|--------------------------------------------
    		rule      "start_state"   "/\[\d+-\d+-\d+T\d+:\d+:\d+.\d+\](.*)/"   "cont"
        rule      "cont"          "/(^\s(.*)|}(.*))/"                       "cont"
    ```
    

### OUTPUT

```bash
[OUTPUT]
    Name                es
    Host                << ES Endpoint >>
    Port                443
    tls                 on
    Logstash_Format     on
    Logstash_DateFormat %Y-%m-%d
    Match               *
    Logstash_Prefix     << Index Name >>
```

최종적으로 정제한 로그를 전송하는 단계입니다.

동일하게 Name 으로 종류를 선택하고 Match 하는 로그를 전송하는 역할을 합니다

- `Logstash_Prefix`, `Logstash_Format`, `Logstash_DateFormat`
    - Log 전송 시, Index 뒤에 붙을 내용을 포맷팅하는 부분입니다
    - 예를 들어 아래와 같이 설정이 되어 있다면
    `Logstash_Prefix` : testIndex
    `Logstash_Format`: on
    `Logstash_DateFormat`: %Y-%m-%d
    실제 인덱스는 `testIndex-YYYY-MM-DD` 형식으로 저장되게 됩니다

+ 추가

Kibana 에서 로그를 보려면 Index 를 Stack 에 추가해야 합니다
추가하는 방법은 아래와 같습니다.

1. Kibana 페이지좌측 상단의 햄버거 메뉴
2. Stack Management
3. Index Patterns
4. Create index pattern
5. Index pattern name 에 추가할 인덱스 패턴을 입력 후 Next sterp 
(ex. testIndex-*)
6. Time field 에 time 을 선택합니다
(선택하지 않으면 로그가 시간순으로 보이지 않게 됩니다)
7. Create iundex pattern

# 참고

---

[https://kubernetes.io/docs/concepts/cluster-administration/logging/](https://kubernetes.io/docs/concepts/cluster-administration/logging/)

[https://docs.fluentbit.io/manual/about/what-is-fluent-bit](https://docs.fluentbit.io/manual/)

[https://aws.amazon.com/ko/blogs/containers/kubernetes-logging-powered-by-aws-for-fluent-bit/](https://aws.amazon.com/ko/blogs/containers/kubernetes-logging-powered-by-aws-for-fluent-bit/)

이상 데이블 강경호이었습니다.

감사합니다.